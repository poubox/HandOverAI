from pydub import AudioSegment
import os
import torch
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
import librosa

# ëª¨ë¸ ì¤€ë¹„
processor = AutoProcessor.from_pretrained("openai/whisper-medium")
model = AutoModelForSpeechSeq2Seq.from_pretrained("openai/whisper-medium")
model.eval()

def split_audio(file_path, chunk_dir="uploads/chunks", chunk_length_ms=30_000):
    # í´ë” ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(chunk_dir, exist_ok=True)
    audio = AudioSegment.from_file(file_path)
    chunks = []

    for i in range(0, len(audio), chunk_length_ms):
        chunk = audio[i:i + chunk_length_ms]
        out_path = os.path.join(chunk_dir, f"chunk_{i // chunk_length_ms}.wav")
        chunk.export(out_path, format="wav")
        chunks.append(out_path)

    return chunks

def transcribe(audio_path):
    speech, sr = librosa.load(audio_path, sr=16000)
    inputs = processor(speech, sampling_rate=16000, return_tensors="pt")
    with torch.no_grad():
        generated_ids = model.generate(inputs["input_features"], max_length=448)
        text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return text

if __name__ == "__main__":
    original_file = "uploads/my_audio.wav"  # âœ… ì›ë³¸ ì˜¤ë””ì˜¤
    chunk_output_dir = "uploads/chunks"     # âœ… ì •ë¦¬ëœ chunk ì €ì¥ ìœ„ì¹˜

    chunks = split_audio(original_file, chunk_output_dir)

    all_text = ""
    for chunk_path in chunks:
        print(f"ğŸŸ¡ ì²˜ë¦¬ ì¤‘: {chunk_path}")
        chunk_text = transcribe(chunk_path)
        print(f"âœ… ë³€í™˜ëœ í…ìŠ¤íŠ¸: {chunk_text}")
        all_text += chunk_text + " "

    print("\nğŸŸ¢ ì „ì²´ ê²°ê³¼:")
    print(all_text)
