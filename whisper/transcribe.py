import sys
from pydub import AudioSegment
import os
import torch
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
import librosa

# Whisper ëª¨ë¸ ì¤€ë¹„
processor = AutoProcessor.from_pretrained("openai/whisper-medium")
model = AutoModelForSpeechSeq2Seq.from_pretrained("openai/whisper-medium")
model.eval()

# ì˜¤ë””ì˜¤ ë¶„í•  í•¨ìˆ˜
def split_audio(file_path, chunk_dir="uploads/chunks", chunk_length_ms=30_000):
    os.makedirs(chunk_dir, exist_ok=True)
    audio = AudioSegment.from_file(file_path)
    chunks = []
    for i in range(0, len(audio), chunk_length_ms):
        chunk = audio[i:i + chunk_length_ms]
        out_path = os.path.join(chunk_dir, f"chunk_{i // chunk_length_ms}.wav")
        chunk.export(out_path, format="wav")
        chunks.append(out_path)
    return chunks

# ë³€í™˜ í•¨ìˆ˜
def transcribe(audio_path):
    speech, sr = librosa.load(audio_path, sr=16000)
    inputs = processor(speech, sampling_rate=16000, return_tensors="pt")
    with torch.no_grad():
        generated_ids = model.generate(inputs["input_features"], max_length=448)
        text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return text

# ì‹¤í–‰ íë¦„
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("âŒ ì‚¬ìš©ë²•: python3 transcribe.py [ì˜¤ë””ì˜¤íŒŒì¼ê²½ë¡œ]")
        sys.exit(1)

    original_file = sys.argv[1]
    chunks = split_audio(original_file)

    all_text = ""
    for chunk_path in chunks:
        print(f"ğŸŸ¡ ì²˜ë¦¬ ì¤‘: {chunk_path}")
        chunk_text = transcribe(chunk_path)
        print(f"âœ… ë³€í™˜ëœ í…ìŠ¤íŠ¸: {chunk_text}")
        all_text += chunk_text + " "

    print("\nğŸŸ¢ ì „ì²´ ê²°ê³¼:")
    print(all_text)

    with open("output.txt", "w", encoding="utf-8") as f:
        f.write(all_text)
